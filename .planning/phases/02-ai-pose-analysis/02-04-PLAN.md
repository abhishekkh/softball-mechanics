---
phase: 02-ai-pose-analysis
plan: 04
type: execute
wave: 3
depends_on: [02-02, 02-03]
files_modified:
  - src/hooks/usePoseAnalysis.ts
  - src/components/review/VideoWithOverlay.tsx
  - src/components/review/AnalysisTimeline.tsx
autonomous: true
requirements: [AI-01, AI-03]

must_haves:
  truths:
    - "usePoseAnalysis initializes Web Worker, samples HLS video at 5fps, runs MediaPipe, posts results to /api/analysis"
    - "Worker is terminated on component unmount — no memory leak"
    - "Analysis only runs when video_analyses.status === 'pending' — skips if already 'complete'"
    - "VideoWithOverlay renders stacked canvas over HLS video; canvas clears and redraws skeleton on every frame scrub"
    - "AnalysisTimeline renders colored markers at flagged frame positions on the scrubber"
  artifacts:
    - path: "src/hooks/usePoseAnalysis.ts"
      provides: "Orchestration hook: worker init, frame sampling, analysis, persistence, progress reporting"
      exports: ["usePoseAnalysis"]
    - path: "src/components/review/VideoWithOverlay.tsx"
      provides: "HLS video + canvas skeleton overlay with toggle"
      exports: ["VideoWithOverlay"]
    - path: "src/components/review/AnalysisTimeline.tsx"
      provides: "Flagged frame markers on scrubber timeline"
      exports: ["AnalysisTimeline"]
  key_links:
    - from: "src/hooks/usePoseAnalysis.ts"
      to: "src/workers/pose-analyzer.worker.ts"
      via: "new Worker(new URL('@/workers/pose-analyzer.worker.ts', import.meta.url), { type: 'module' })"
      pattern: "pose-analyzer.worker"
    - from: "src/hooks/usePoseAnalysis.ts"
      to: "/api/analysis"
      via: "fetch POST after analysis completes"
      pattern: "fetch.*api/analysis"
    - from: "src/components/review/VideoWithOverlay.tsx"
      to: "src/lib/pose/landmarks.ts"
      via: "drawSkeleton(ctx, frameData.landmarks, ...)"
      pattern: "drawSkeleton"
---

<objective>
Build the usePoseAnalysis orchestration hook and the two display components that consume its output — VideoWithOverlay (video + canvas skeleton) and AnalysisTimeline (flagged frame markers on scrubber).

Purpose: The hook is the engine that ties Web Worker, API persistence, and progress reporting together. The components render the results. Plan 05 adds the sidebar and wires the review page.
Output: usePoseAnalysis hook, VideoWithOverlay, AnalysisTimeline.
</objective>

<execution_context>
@/Users/abhishekhodavdekar/.claude/get-shit-done/workflows/execute-plan.md
@/Users/abhishekhodavdekar/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/02-ai-pose-analysis/02-RESEARCH.md
@.planning/phases/02-ai-pose-analysis/02-02-SUMMARY.md
@.planning/phases/02-ai-pose-analysis/02-03-SUMMARY.md

<interfaces>
<!-- From src/types/analysis.ts (Plan 01) -->
```typescript
export type AnalysisStatus = 'pending' | 'analyzing' | 'complete' | 'error' | 'low_confidence'
export interface NormalizedLandmark { x: number; y: number; z: number; visibility: number }
export interface MechanicsFlag { issue: string; confidence: number; severity: 'warning'|'error'; jointIndices: number[] }
export interface FrameAngles { elbowSlotDeg: number|null; shoulderTiltDeg: number|null; hipRotationDeg: number|null }
export interface FrameAnalysis { frameIndex: number; timestampMs: number; landmarks: NormalizedLandmark[]; angles: FrameAngles; flags: MechanicsFlag[] }
export interface VideoAnalysis { id: string; videoId: string; status: AnalysisStatus; progressPct: number; ... }
export interface AnalysisPayload { videoId: string; frames: FrameAnalysis[]; framingWarning?: string }
```

<!-- From src/lib/pose/landmarks.ts (Plan 02) -->
```typescript
export function drawSkeleton(
  ctx: CanvasRenderingContext2D,
  landmarks: NormalizedLandmark[],
  canvasWidth: number,
  canvasHeight: number,
  flaggedIndices?: Set<number>
): void
```

<!-- From src/lib/pose/angles.ts (Plan 02) -->
```typescript
export function computeFrameAngles(landmarks: NormalizedLandmark[], handedness?: 'right'|'left'): FrameAngles
```

<!-- From src/lib/pose/flags.ts (Plan 02) -->
```typescript
export function flagMechanics(elbowSlot, shoulderTilt, hipRotation, landmarks): MechanicsFlag[]
export function checkFramingQuality(landmarks): string | null
```

<!-- HLS player pattern from Phase 1 (Plan 04): -->
<!-- HLSPlayer uses hls.js, videoRef exposed via forwardRef -->
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: usePoseAnalysis hook — Web Worker orchestration and frame sampling</name>
  <files>src/hooks/usePoseAnalysis.ts</files>
  <action>
Create `src/hooks/usePoseAnalysis.ts`:

The hook:
1. On mount: fetches existing `video_analyses` row from Supabase for the videoId
2. If status is 'complete' or 'error': loads stored frames from `video_analysis_frames`, returns them — no analysis runs
3. If status is 'pending': starts the analysis pipeline
4. Analysis pipeline: (a) init worker, (b) sample video at 5fps, (c) run MediaPipe on each frame, (d) compute angles + flags, (e) POST to /api/analysis, (f) terminate worker
5. Exposes: `{ frames, analysisStatus, progressPct, currentFrameData, framingWarning, startReanalysis }`

```typescript
// src/hooks/usePoseAnalysis.ts
'use client'

import { useState, useEffect, useRef, useCallback } from 'react'
import * as Comlink from 'comlink'
import { createBrowserClient } from '@supabase/ssr'
import { computeFrameAngles } from '@/lib/pose/angles'
import { flagMechanics, checkFramingQuality } from '@/lib/pose/flags'
import type { FrameAnalysis, VideoAnalysis, AnalysisStatus, NormalizedLandmark } from '@/types/analysis'

const SAMPLE_FPS = 5   // Analyze one frame every 200ms of video

interface UsePoseAnalysisResult {
  frames: FrameAnalysis[]
  analysisStatus: AnalysisStatus | null
  progressPct: number
  framingWarning: string | null
  startReanalysis: () => void
}

export function usePoseAnalysis(
  videoId: string,
  videoRef: React.RefObject<HTMLVideoElement | null>
): UsePoseAnalysisResult {
  const [frames, setFrames] = useState<FrameAnalysis[]>([])
  const [analysisStatus, setAnalysisStatus] = useState<AnalysisStatus | null>(null)
  const [progressPct, setProgressPct] = useState(0)
  const [framingWarning, setFramingWarning] = useState<string | null>(null)
  const [triggerAnalysis, setTriggerAnalysis] = useState(0)

  const workerRef = useRef<Worker | null>(null)
  const abortRef = useRef(false)

  const supabase = createBrowserClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY!
  )

  const loadStoredFrames = useCallback(async () => {
    const { data, error } = await supabase
      .from('video_analysis_frames')
      .select('*')
      .eq('video_id', videoId)
      .order('frame_index', { ascending: true })

    if (error || !data) return

    const loaded: FrameAnalysis[] = data.map((row) => ({
      frameIndex: row.frame_index as number,
      timestampMs: row.timestamp_ms as number,
      landmarks: row.landmarks as NormalizedLandmark[],
      angles: {
        elbowSlotDeg: row.elbow_slot_deg as number | null,
        shoulderTiltDeg: row.shoulder_tilt_deg as number | null,
        hipRotationDeg: row.hip_rotation_deg as number | null,
      },
      flags: (row.flags ?? []) as ReturnType<typeof flagMechanics>,
    }))
    setFrames(loaded)
  }, [videoId, supabase])

  const runAnalysis = useCallback(async () => {
    const video = videoRef.current
    if (!video) return

    abortRef.current = false
    setAnalysisStatus('analyzing')
    setProgressPct(0)

    // Mark analyzing in DB to gate Re-analyze button
    await supabase
      .from('video_analyses')
      .update({ status: 'analyzing', progress_pct: 0 })
      .eq('video_id', videoId)

    // Init worker
    const worker = new Worker(
      new URL('@/workers/pose-analyzer.worker.ts', import.meta.url),
      { type: 'module' }
    )
    workerRef.current = worker
    const api = Comlink.wrap<{
      init: () => Promise<void>
      detectOnImageBitmap: (b: ImageBitmap) => Promise<{ landmarks: Array<Array<{ x: number; y: number; z: number; visibility: number }>> }>
    }>(worker)

    try {
      await api.init()
    } catch (err) {
      console.error('[usePoseAnalysis] Worker init failed:', err)
      setAnalysisStatus('error')
      await supabase
        .from('video_analyses')
        .update({ status: 'error', error_message: String(err) })
        .eq('video_id', videoId)
      worker.terminate()
      return
    }

    // Wait for video metadata
    await new Promise<void>((resolve) => {
      if (video.readyState >= 1) { resolve(); return }
      video.addEventListener('loadedmetadata', () => resolve(), { once: true })
    })

    const duration = video.duration
    const frameInterval = 1 / SAMPLE_FPS   // seconds between sampled frames
    const totalFrames = Math.floor(duration * SAMPLE_FPS)
    const collectedFrames: FrameAnalysis[] = []
    let detectedFramingWarning: string | null = null

    const offscreen = new OffscreenCanvas(video.videoWidth || 1280, video.videoHeight || 720)
    const offCtx = offscreen.getContext('2d')!

    for (let i = 0; i < totalFrames; i++) {
      if (abortRef.current) break

      const timestampS = i * frameInterval
      video.currentTime = timestampS

      // Wait for seeked event
      await new Promise<void>((resolve) => {
        video.addEventListener('seeked', () => resolve(), { once: true })
      })

      // Draw current frame to offscreen canvas
      offCtx.drawImage(video, 0, 0, offscreen.width, offscreen.height)
      const bitmap = offscreen.transferToImageBitmap()

      let result: { landmarks: Array<Array<{ x: number; y: number; z: number; visibility: number }>> }
      try {
        result = await api.detectOnImageBitmap(bitmap)
      } catch {
        continue  // Skip frame on inference error
      }

      const rawLandmarks = result.landmarks[0]
      if (!rawLandmarks || rawLandmarks.length === 0) continue

      const landmarks = rawLandmarks as NormalizedLandmark[]
      const angles = computeFrameAngles(landmarks)
      const frameFlags = flagMechanics(
        angles.elbowSlotDeg,
        angles.shoulderTiltDeg,
        angles.hipRotationDeg,
        landmarks
      )

      // Check framing quality on first valid frame only
      if (i === 0 && !detectedFramingWarning) {
        detectedFramingWarning = checkFramingQuality(landmarks)
      }

      collectedFrames.push({
        frameIndex: i,
        timestampMs: Math.round(timestampS * 1000),
        landmarks,
        angles,
        flags: frameFlags,
      })

      const pct = Math.round(((i + 1) / totalFrames) * 100)
      setProgressPct(pct)
      // Batch UI update every 10 frames to avoid excessive re-renders
      if (i % 10 === 0) {
        setFrames([...collectedFrames])
      }
    }

    worker.terminate()
    workerRef.current = null

    if (abortRef.current) return

    setFramingWarning(detectedFramingWarning)
    setFrames(collectedFrames)

    // Persist to Supabase via API route
    try {
      const response = await fetch('/api/analysis', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          videoId,
          frames: collectedFrames,
          framingWarning: detectedFramingWarning ?? undefined,
        }),
      })

      if (response.ok) {
        setAnalysisStatus(collectedFrames.length > 0 ? 'complete' : 'low_confidence')
        setProgressPct(100)
      } else {
        setAnalysisStatus('error')
      }
    } catch (err) {
      console.error('[usePoseAnalysis] Persist failed:', err)
      setAnalysisStatus('error')
    }
  }, [videoId, videoRef, supabase])

  useEffect(() => {
    if (!videoId) return

    const initialize = async () => {
      // Check existing analysis status
      const { data: existing } = await supabase
        .from('video_analyses')
        .select('status, progress_pct, framing_warning')
        .eq('video_id', videoId)
        .single()

      if (existing) {
        const status = existing.status as AnalysisStatus
        setAnalysisStatus(status)
        setFramingWarning(existing.framing_warning as string | null)

        if (status === 'complete' || status === 'low_confidence') {
          await loadStoredFrames()
          return
        }

        if (status === 'error') {
          return  // Wait for manual re-analysis trigger
        }
      }

      // pending, analyzing, or no row: run analysis
      if (triggerAnalysis >= 0) {
        await runAnalysis()
      }
    }

    initialize()

    return () => {
      abortRef.current = true
      workerRef.current?.terminate()
      workerRef.current = null
    }
  }, [videoId, triggerAnalysis]) // eslint-disable-line react-hooks/exhaustive-deps

  const startReanalysis = useCallback(async () => {
    // Reset DB status to pending, then re-run
    await supabase
      .from('video_analyses')
      .update({ status: 'pending', progress_pct: 0 })
      .eq('video_id', videoId)
    setFrames([])
    setProgressPct(0)
    setAnalysisStatus('pending')
    setTriggerAnalysis((n) => n + 1)
  }, [videoId, supabase])

  return { frames, analysisStatus, progressPct, framingWarning, startReanalysis }
}
```
  </action>
  <verify>
    <automated>npx tsc --noEmit 2>&1 | head -20</automated>
  </verify>
  <done>
    - src/hooks/usePoseAnalysis.ts exports usePoseAnalysis
    - Hook checks for existing complete analysis before running Worker
    - Worker terminated on component unmount (abortRef + worker.terminate())
    - Progress reported every 10 frames via setProgressPct
    - startReanalysis resets DB status to pending and increments trigger counter
    - npx tsc --noEmit exits 0
  </done>
</task>

<task type="auto">
  <name>Task 2: VideoWithOverlay and AnalysisTimeline components</name>
  <files>
    src/components/review/VideoWithOverlay.tsx
    src/components/review/AnalysisTimeline.tsx
  </files>
  <action>
**Create src/components/review/VideoWithOverlay.tsx**

```typescript
// src/components/review/VideoWithOverlay.tsx
// HLS video player with stacked canvas skeleton overlay
// Canvas is positioned absolutely over the video (pointer-events: none)
'use client'

import { useRef, useEffect, forwardRef, useImperativeHandle } from 'react'
import Hls from 'hls.js'
import { drawSkeleton } from '@/lib/pose/landmarks'
import type { FrameAnalysis, MechanicsFlag } from '@/types/analysis'

export interface VideoWithOverlayHandle {
  videoElement: HTMLVideoElement | null
}

interface Props {
  hlsUrl: string
  frames: FrameAnalysis[]
  showSkeleton: boolean
  onTimeUpdate?: (currentTimeSec: number) => void
}

// Build a lookup map from timestamp_ms to FrameAnalysis for O(1) frame access
function buildFrameMap(frames: FrameAnalysis[]): Map<number, FrameAnalysis> {
  const map = new Map<number, FrameAnalysis>()
  for (const f of frames) {
    map.set(f.timestampMs, f)
  }
  return map
}

// Find the nearest frame for a given time in milliseconds (±200ms window)
function findNearestFrame(
  frames: FrameAnalysis[],
  timMs: number
): FrameAnalysis | null {
  if (frames.length === 0) return null
  let nearest = frames[0]
  let minDiff = Math.abs(frames[0].timestampMs - timMs)
  for (const f of frames) {
    const diff = Math.abs(f.timestampMs - timMs)
    if (diff < minDiff) {
      minDiff = diff
      nearest = f
    }
  }
  // Only return if within 300ms of current time (1.5 frames at 5fps)
  return minDiff <= 300 ? nearest : null
}

export const VideoWithOverlay = forwardRef<VideoWithOverlayHandle, Props>(
  function VideoWithOverlay({ hlsUrl, frames, showSkeleton, onTimeUpdate }, ref) {
    const videoRef = useRef<HTMLVideoElement>(null)
    const canvasRef = useRef<HTMLCanvasElement>(null)
    const hlsRef = useRef<Hls | null>(null)

    useImperativeHandle(ref, () => ({
      videoElement: videoRef.current,
    }))

    // Initialize HLS player
    useEffect(() => {
      const video = videoRef.current
      if (!video || !hlsUrl) return

      if (Hls.isSupported()) {
        const hls = new Hls()
        hlsRef.current = hls
        hls.loadSource(hlsUrl)
        hls.attachMedia(video)
      } else if (video.canPlayType('application/vnd.apple.mpegurl')) {
        // Safari: native HLS support
        video.src = hlsUrl
      }

      return () => {
        hlsRef.current?.destroy()
        hlsRef.current = null
      }
    }, [hlsUrl])

    // Sync canvas size to video dimensions on load
    useEffect(() => {
      const video = videoRef.current
      const canvas = canvasRef.current
      if (!video || !canvas) return

      const syncSize = () => {
        canvas.width = video.videoWidth || video.clientWidth
        canvas.height = video.videoHeight || video.clientHeight
        canvas.style.width = `${video.clientWidth}px`
        canvas.style.height = `${video.clientHeight}px`
      }

      video.addEventListener('loadedmetadata', syncSize)
      return () => video.removeEventListener('loadedmetadata', syncSize)
    }, [])

    // Draw skeleton on timeupdate — fires as video plays or is scrubbed
    useEffect(() => {
      const video = videoRef.current
      const canvas = canvasRef.current
      if (!video || !canvas) return

      const handleTimeUpdate = () => {
        const timMs = Math.round(video.currentTime * 1000)
        onTimeUpdate?.(video.currentTime)

        const ctx = canvas.getContext('2d')
        if (!ctx) return
        ctx.clearRect(0, 0, canvas.width, canvas.height)

        if (!showSkeleton || frames.length === 0) return

        const frameData = findNearestFrame(frames, timMs)
        if (!frameData || frameData.landmarks.length === 0) return

        // Collect flagged joint indices for red highlight
        const flaggedIndices = new Set<number>(
          frameData.flags.flatMap((f: MechanicsFlag) => f.jointIndices)
        )

        drawSkeleton(ctx, frameData.landmarks, canvas.width, canvas.height, flaggedIndices)
      }

      video.addEventListener('timeupdate', handleTimeUpdate)
      return () => video.removeEventListener('timeupdate', handleTimeUpdate)
    }, [frames, showSkeleton, onTimeUpdate])

    return (
      <div className="relative inline-block w-full">
        <video
          ref={videoRef}
          className="w-full block"
          controls
          playsInline
        />
        <canvas
          ref={canvasRef}
          className="absolute top-0 left-0 pointer-events-none"
        />
      </div>
    )
  }
)
```

**Create src/components/review/AnalysisTimeline.tsx**

```typescript
// src/components/review/AnalysisTimeline.tsx
// Renders color-coded markers on a progress bar for flagged frames
// Clicking a marker seeks the video to that frame

'use client'

import type { FrameAnalysis } from '@/types/analysis'

interface Props {
  frames: FrameAnalysis[]
  videoDurationSec: number
  currentTimeSec: number
  onSeek: (timeSec: number) => void
}

export function AnalysisTimeline({ frames, videoDurationSec, currentTimeSec, onSeek }: Props) {
  if (frames.length === 0 || videoDurationSec === 0) return null

  // Only render frames that have at least one flag
  const flaggedFrames = frames.filter((f) => f.flags.length > 0)

  return (
    <div className="relative w-full h-8 bg-neutral-800 rounded overflow-hidden mt-2">
      {/* Playhead indicator */}
      <div
        className="absolute top-0 h-full w-0.5 bg-white z-10 pointer-events-none"
        style={{ left: `${(currentTimeSec / videoDurationSec) * 100}%` }}
      />

      {/* Flagged frame markers */}
      {flaggedFrames.map((frame) => {
        const positionPct = (frame.timestampMs / 1000 / videoDurationSec) * 100
        const hasError = frame.flags.some((f) => f.severity === 'error')

        return (
          <button
            key={frame.frameIndex}
            type="button"
            className={`absolute top-1 h-6 w-1.5 rounded-sm cursor-pointer transition-opacity hover:opacity-100 opacity-80 ${
              hasError ? 'bg-red-500' : 'bg-yellow-400'
            }`}
            style={{ left: `${positionPct}%`, transform: 'translateX(-50%)' }}
            title={frame.flags.map((f) => `${f.issue} (${Math.round(f.confidence * 100)}%)`).join(', ')}
            onClick={() => onSeek(frame.timestampMs / 1000)}
            aria-label={`Seek to flagged frame at ${(frame.timestampMs / 1000).toFixed(1)}s`}
          />
        )
      })}
    </div>
  )
}
```
  </action>
  <verify>
    <automated>npx tsc --noEmit 2>&1 | head -20</automated>
  </verify>
  <done>
    - src/components/review/VideoWithOverlay.tsx exports VideoWithOverlay (forwardRef)
    - VideoWithOverlay uses hls.js for HLS playback; canvas stacked absolutely over video
    - AnalysisTimeline renders flagged markers; red for severity:error, yellow for severity:warning
    - Clicking a marker calls onSeek(timeSec)
    - npx tsc --noEmit exits 0
  </done>
</task>

</tasks>

<verification>
- src/hooks/usePoseAnalysis.ts exports usePoseAnalysis
- src/components/review/VideoWithOverlay.tsx exports VideoWithOverlay with forwardRef
- src/components/review/AnalysisTimeline.tsx exports AnalysisTimeline
- Worker instantiation uses `new Worker(new URL('@/workers/pose-analyzer.worker.ts', import.meta.url), { type: 'module' })`
- Worker is terminated in useEffect cleanup
- npx tsc --noEmit exits 0
</verification>

<success_criteria>
After Phase 2 Plan 04:
1. usePoseAnalysis correctly short-circuits analysis when existing results exist (complete/low_confidence)
2. VideoWithOverlay renders HLS video with canvas overlay; skeleton drawn from stored frame data
3. AnalysisTimeline shows flag markers — red for error severity, yellow for warning severity
4. TypeScript compiles clean
</success_criteria>

<output>
After completion, create `.planning/phases/02-ai-pose-analysis/02-04-SUMMARY.md`
</output>
